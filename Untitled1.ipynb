{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauriPatange/test-repo/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z08jR3EbK1T",
        "colab_type": "text"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1K5YOfDVf6Fq7cNE11ku_dXjjDYyaKhC6'width=\"200\" height=\"100\"/>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "<center>\n",
        "<font size=\"5\">Machine Learning </font>\n",
        "<br>\n",
        "  <br>\n",
        "Spring 2019\n",
        "  <br>\n",
        "  <br>\n",
        "\n",
        "\n",
        "**Assessment 2**\n",
        "<br>\n",
        "  <br>\n",
        "  \n",
        "\n",
        "<font size=\"4\">**CREDIT CARD FRAUD DETECTION USING RANDOM FOREST ALGORITHM**</font>\n",
        "\n",
        "<br>\n",
        "  <br>\n",
        "  \n",
        "  <U>Presented by</U>\n",
        "\n",
        "Gauri Patange [13173505]\n",
        "\n",
        "Smita Naik [13319684]\n",
        "  </center>\n",
        "  \n",
        "<p style=\"page-break-after: always;\">&nbsp;</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y1E0RS5QTeZ",
        "colab_type": "text"
      },
      "source": [
        "# INTRODUCTION\n",
        "\n",
        "Nowadays, every person has credit card through which anywhere they can spend money. Starting in 1980, there has been drastic increment of credit card, debit card use. Credit card is nothing, but a small plastic card provided or issued by bank which help to purchase goods and services on credit. In many countries, the use of credit card is massive which leading to misuse the details of credit card. With this many frauds are happening in many countries. Credit card fraud is a wide term use for theft committed using or involving a payment card. Fraud detection of card starts with either theft or the data associated with account such as card number, or other information that necessarily available during the transaction. Through this project we are trying to solve the problem that many companies are facing today. \n",
        "\n",
        "The idea of this project is to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase. So, to solve this problem, we have taken the huge dataset of Europe country which contain transaction made by credit cards in September 2013.The dataset is highly unbalanced where it presents transaction that occurred in two days. In 2 days of transactions, many frauds were happened. The main aim of this project is to identify fraudulent credit card transactions.  \n",
        "\n",
        "How this credit card fraud happens? When user give their credit card details including credit card number, their CVV to unfamiliar or unknown person, when card is stolen or lost or when mail is diverted from recipient and taken by criminal.\n",
        "\n",
        "There are 11 most common forms of credit card fraud\n",
        "1.\tApplication fraud\n",
        "2.\tManual credit card imprints\n",
        "3.\tCNP (Card not present) Fraud\n",
        "4.\tCounterfeit Card Fraud\n",
        "5.\tLost or stolen card fraud\n",
        "6.\tCard ID theft\n",
        "7.\tMail Non-Receipt card fraud\n",
        "8.\tAssumed Identity\n",
        "9.\tFake Cards\n",
        "10.\tDoctored Cards\n",
        "11.\tAccount takeover \n",
        "\n",
        "Approaches for the project are : \n",
        "\n",
        "1. Randomly split the dataset into train, validation, and test set.\n",
        "2. Predict and evaluate with validation set.\n",
        "3. Resample the dataset.\n",
        "4. Train on resampled train set then predict and evaluate with validation set.\n",
        "5. Try other different models.\n",
        "6. Compare the difference between the predictions and choose the best model.\n",
        "7. Find the optimised threshold of the chosen model.\n",
        "8. Predict on test set to report final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3h8OnIkRNVe",
        "colab_type": "text"
      },
      "source": [
        "# EXPLORATION\n",
        "\n",
        "## DATA SOURCE\n",
        "\n",
        "In this report, we tried to identify fraudulent data in the credit card transaction. So, firstly, we searched suitable dataset in the Kaggle which consists transaction of two days. The dataset found in the Kaggle which has 31 columns. Dataset is made by credit card transaction in September 2013 by European cardholder. This dataset has 492 frauds out of 284,807 transactions. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knHDfRo1RmCW",
        "colab_type": "text"
      },
      "source": [
        "## DATA DESCRIPTION\n",
        "\n",
        "* **‘Class’** is the target value, and it has a binary value can have either zero or one. In which zero refers to not fraud and one to fraud. \n",
        "\n",
        "* **‘Amount’** is the amount of transaction. \n",
        "\n",
        "* **‘Time’** is the time of the transaction. \n",
        "\n",
        "* The rest of the attributes are noted as **‘V1 to V28’** and these features are anonymized. \n",
        "\n",
        "*\tthe data is highly unbalanced concerning the Class variables. \n",
        "\n",
        "*\tThere consists only 0.172% of rows which has class =1. In this case, we identified to preserve the data unbalancing or oversampling which consists data with minority value of target data otherwise under sampling where the data with majority value of targeted value. \n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=16f_ZnaNaKnfsAcXWynVYtRo3HkipDMus'/>\n",
        "<figcaption> Figure 1. fraudulent credit card transactions</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br>\n",
        "\n",
        "Figure 1 is the screenshot of the given dataset. Initially before implementing random forest algorithm, we pre-processing the dataset to eliminate the minor errors like data cleaning, normalization, transformation, feature extraction and selection. In the data cleaning, we cleaned the data in KNIME platform where we eliminated missing values, error values, double values etc. and also normalized the data by using 0,1 values. After pre-processing the data, we applied random forest algorithm. By using pre-processing technique, the result of the algorithm will provide more accurate.\n",
        "\n",
        "we tried to preserve unbalancing of the data. For validating the result, we used confusion matrix or accuracy. furthermore, for relevant validation we tried alternative solution using AUC. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSvkGb-cYNXj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# METHODOLOGY\n",
        "\n",
        "In this report, we chose random Forest is an ensemble algorithm as a classifier which is used to build predictive models for both regression and classification. In Random forest algorithm, the model creates a forest of uncorrelated decision trees. Then it combines those trees to improve the performance of the training dataset randomly. This approach will improve the prediction of accuracy. Random Forest utilizes a bootstrapping algorithm in which decision tree will be trained with different data subsamples.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1sv-HTqtVNkQ1LS02YW0rwuZBKLKd5cqc' />\n",
        "<figcaption> Figure 2. Spliting process of data into test set and training set  </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "Random subset features are used in the Random Forest. Considering an example of 50 features in the data, random forest itself chooses a set of attributes of them let us take 10 to train each tree. Thus, each tree will train 10   random features. This will be used in training to find the best split of each node tree. After having the set of decision tree as shown in the figure below, the result of each tree will be combined to get final outcome. This phase is technically termed as vote. Here multiple decision tree is used to train with different subset and to make decisions. \n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=198uI_UDaXf9P45g_XOgVvDGHzh6eStHH'/>\n",
        "<figcaption> Figure 3. Process of Random Forest Algorithm </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Data Input\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1i4PFOc3x0RwA6tZ0mMeg8JAtNJHN2hZv'/>\n",
        "<figcaption> Figure 4. Input of Dataset </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1pCLlHtBKhNMHb77eK_zoK-UzzZioih6a'/>\n",
        "<figcaption> Figure 5. Count of 0 and 1 </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "In the above figure 5, values of 0 and 1 are identified, in this value of 0 is 284315 and 1 is 492 which stated that the two set are imbalanced. So we split the data into two set that is training set and testing set for exploratory analysis.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1U1mdil4YaNFYOzZra_85GCHeJ5I3cwOq'/>\n",
        "<figcaption> Figure 6.Data split process </figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEYk6psBaxuS",
        "colab_type": "text"
      },
      "source": [
        "# EVALUATION\n",
        "\n",
        "In this section, we are implementing different data models and predicting the data with training set. so here we are first using logical regression to predict the probability of labels.\n",
        "We use area under Precision-Recall curve, aka average precision score to evaluate the performance. The reason to choose PR AUC over ROC AUC is that PR does not account for true negatives, therefore more suitable for imbalanced classification.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1k5nEZL2d8rLxiRAPa5dE8xWBRINccTLl'/>\n",
        "<figcaption> Figure 7.Logical Regression</figcaption></center>\n",
        "</figure>\n",
        "through this, we will predict the model with validation set. output will be the probability it will analyze with respect to PR curve.\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1AoJ07j2B0ONqD_d9dWSjxmuL02Gj4H3A'/>\n",
        "<figcaption> Figure 8.prediction of model with validation set</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1Mj4MHzstVUJParajKzzVDH0MJ97oS8rj'/>\n",
        "<figcaption> Figure 9.PR Curve </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Data Sampling\n",
        "In this process, we resample the data of train set which will generate new sample result.\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1WnBAVJ9f6H-uPoPVTqVWAeOq8T6fdq7C'/>\n",
        "<figcaption> Figure 10.Data Resampling </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "##Predict and Evaluate the resampled set\n",
        "Here we will use logical regression model to test performance on same validation set\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1p2j2bYHXtNmK3vMQAiaRoTNhpEEHHBFc'/>\n",
        "<figcaption> Figure 11.Logical Regression using resample dataset </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1whQSg_oKcF1cmdRvWF-OOZmQmBOIN01o'/>\n",
        "<figcaption> Figure 12.Prediction using resample dataset </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1eu4V4BGFQ67uPvhNR73woxTK3szehb9P'/>\n",
        "<figcaption> Figure 13.PR Curve </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Through this resampling, we can see the improvement to our model. next we will compare the different data model to see if there is more improvement or not.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Random Forest - Resampled set\n",
        "now here we are using random forest algorithm on resample dataset to see the difference in improvement. this algorithm have satisfying performance on classification problem\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=15DekcTHuAmMmtfdRSvnpLvk2eVqTRbNg'/>\n",
        "<figcaption> Figure 14.Random Forest Classifier </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=12K8H0O8tXO-thGV029iEdSzU_9ADh3yH'/>\n",
        "<figcaption> Figure 15.Prediction using resample dataset </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1JMPq6f6N_yi8ktSI0S0RZ9NAsfnjEuKv'/>\n",
        "<figcaption> Figure 16.PR Curve </figcaption></center>\n",
        "</figure>\n",
        "In the above result we can clearly say that random forest algorithm is suiting more for our data as compare to logical regression. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Cluster Before Prediction - Original set\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1gZk0B9xcGzqgDs0MxefUL1rmNBjMiDPe'/>\n",
        "<figcaption> Figure 17.Clustering on original set </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1yKRrkGWzXZsbSvHW1Y2UwLlN-tDA18__'/>\n",
        "<figcaption> Figure 18.predication process </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1jclWHlyiC-kjUKtX2mov_uz0Xt2pkCzL'/>\n",
        "<figcaption> Figure 19.Prediction Process </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1yInhewqPzkW4_l_Xd_BSkC2-fpSVu4bX'/>\n",
        "<figcaption> Figure 20.PR Curve</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This algorithm is giving the same result as random forest on resampled dataset. so here we are choosing random forest algorith as final algorith because this algorithm is taking less time as compare to clustered ramdom forest\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Threshold and Confusion Matrix\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1SHN5DVRlNYqMVaLcGKJXl0EcwLC1jfQI'/>\n",
        "<figcaption> Figure 21.Data split process </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=17NiiISz93Uuy_xAdjkic3-j4SFAWH7qa'/>\n",
        "<figcaption> Figure 22.Data split process </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "##Predict on Test set\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1I4IcIz4H1sIWqpti-H0CK6ExuVKwOpmG'/>\n",
        "<figcaption> Figure 23.Data split process </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://drive.google.com/uc?id=1Q1XN96qn4VsFVtk4DOkTJzfL7G2PxshI'/>\n",
        "<figcaption> Figure 24.Data split process </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Using our chosen model and threshold on test set, we yield similiar result comparing to the validation set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw8aTEPV1eKR",
        "colab_type": "text"
      },
      "source": [
        "#CONCLUSION\n",
        "\n",
        "Through this project we compared and investigated different data models like linear regression, random forest with resampled data, clustered random forest. we investigated whether resampling dataset can increase the improvement to imbalanced classfication. \n",
        "Therefore, In this project we conclude that, Ramdom Forest algorithm with resampling datset has provided  highest accuracy as compare to other data models.\n",
        "There are other oversample techniques such as ROSE that we didn't try but can possibly improve our model. Undersampling techniques might be a suitable choice too. \n",
        "In real world situation, the optimised point depends. Sometimes the cost of allowing more false positive is a lot cheaper than the loss of not identifying the positive ones. In such case, we can lower our precision to gain more recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msk2yNdY4tYl",
        "colab_type": "text"
      },
      "source": [
        "# ETHICAL\n",
        "\n",
        "Which algorithm comes first in mind? for me it is random forest alogirthm. because When faced with the problem of over-fitting, unbalanced classification the machine learning technique that comes to rescue is random forest again. Random forest is best for prediction of dataset because it takes very less time to predict the data. this techniqu always remain easy and very useful technique.\n",
        "\n"
      ]
    }
  ]
}